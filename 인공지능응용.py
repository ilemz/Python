# -*- coding: utf-8 -*-
"""인공지능응용.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yeudr393d441uwULYQI3NTCX2TqnIlvb

2025.03.11

Jupyter Hello World 입니다
"""

print("Hello World")

"""이미지 테스트입니다"""

import cv2
from google.colab.patches import cv2_imshow
img_path = 'test_001.png'
img = cv2.imread(img_path)
cv2_imshow(img)

!pip install matplotlib

import torch
print(torch.__version__)

if torch.cuda.is_available():
  device = torch.device('cuda:0')
else:
  device = torch.device('cpu')

print(device)

import numpy as np
def AND (x1, x2):
  x=np.array([x1, x2])
  w=np.array([0.5, 0.5])
  b=-0.7
  tmp = np.sum(w*x)+b
  if tmp > 0:
    return 1
  else:
    return 0

"""2025.03.18"""

import numpy as np
def OR (x1, x2):
  x=np.array([x1, x2])
  w=np.array([0.5, 0.5])
  b=-0.2
  tmp = np.sum(w*x)+b
  if tmp > 0:
    return 1
  else:
    return 0

import numpy as np
def NAND (x1, x2):
  x=np.array([x1, x2])
  w=np.array([0.5, 0.5])
  b=-0.7
  tmp = np.sum(w*x)+b
  if tmp > 0:
    return 1
  else:
    return 0

def XOR(x1, x2):
  s1 = NAND(x1, x2)
  s2 = OR(x1, x2)
  y = AND(s1, s2)
  return y

def step_function(x):
  return 1 if x >= 0 else 0

class Perceptron:
  def __init__(self, input_size, lr=0.1, epochs = 10):
    self.weights = np.zeros(input_size)
    self.bias = 0
    self.lr = lr
    self.epochs = epochs

  def predict(self, x):
    linear_output = np.dot(x, self.weights) + self.bias
    return step_function(linear_output)

  def train(self, X, y):
    for epochs in range(self.epochs):
      for i in range(len(X)):
        y_pred = self.predict(X[i])
        error = y[i] - y_pred
        self.weights += self.lr * error * X[i]
        self.bias += self.lr * error

      print(f"Epochs {epochs+1}: Weights = {self.weights}, Bias = {self.bias}, Error = {error}")

X = np.array([[0, 0], [0, 1], [1, 0], [1,1]])
y = np.array([0, 0, 0, 1])

perceptron = Perceptron(input_size=2, lr = 0.1, epochs=10) #lr이 0.01이 되면 epochs는 100으로, lr이 0.5가 되면 epochs는 50
perceptron.train(X, y)

print("\n[학습 결과]")
print(f"Weights: {perceptron.weights}, Bias: {perceptron.bias}")

print("\n[게이트 테스트]")
for inputs in X:
  print(f"입력: {inputs}, 출력: {perceptron.predict(inputs)}")

X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y_nand = np.array([1, 1, 1, 0])
y_and = np.array([0, 0, 0, 1])
y_or = np.array([0, 1, 1, 1])

perceptron_nand = Perceptron(input_size=2, lr = 0.1, epochs=10)
perceptron_nand.train(X, y_nand)
weights_nand = perceptron_nand.weights
bias_nand = perceptron_nand.bias

perceptron_and = Perceptron(input_size=2, lr = 0.1, epochs=10)
perceptron_and.train(X, y_and)
weights_and = perceptron_and.weights
bias_and = perceptron_and.bias

perceptron_or = Perceptron(input_size=2, lr = 0.1, epochs=10)
perceptron_or.train(X, y_or)
weights_or = perceptron_or.weights
bias_or = perceptron_or.bias

def XOR(x):
  nand_output = step_function(np.dot(x, weights_nand) + bias_nand)
  or_output = step_function(np.dot(x, weights_or) + bias_or)
  and_input = np.array([nand_output, or_output])
  #print(f"입력 : {x}, nand_output: {nand_output}, and_input: {and_input}, or_output: {or_output}")
  xor_output = step_function(np.dot(and_input, weights_and) + bias_and)
  return xor_output

for inputs in X:
  print(f"입력: {inputs}, 출력: {XOR(inputs)}")

"""2025.03.25"""

class MulLayer:
  def __init__(self):
    self.x = None
    self.y = None

  def forward(self, x, y):
    self.x = x
    self.y = y
    out = x * y
    return out

  def backward(self, dout):
    dx = dout * self.y
    dy = dout * self.x
    return dx, dy

apple = 100
apple_num = 2
tax = 1.1

mul_apple_layer = MulLayer()
mul_tax_layer = MulLayer()

apple_price = mul_apple_layer.forward(apple, apple_num)
price = mul_tax_layer.forward(apple_price, tax)
print(price)

dprice = 1
dapple_price, dtax = mul_tax_layer.backward(dprice)
dapple, dapple_num = mul_apple_layer.backward(dapple_price)
print(dapple, apple_num, dtax)

from google.colab import drive
drive.mount('/content/drive')

import sys, os
if not os.path.exists("dataset/mnist.py"):
  !wget -P dataset https://raw.githubusercontent.com/jskimn/aiservice_lesson/main/NNBasic/005_minist/dataset/mnist.py

if not os.path.exists("dataset/train-images-idx3-ubyte.gz"):
  !wget -P dataset https://raw.githubusercontent.com/jskimn/aiservice_lesson/main/NNBasic/005_minist/dataset/train-images-idx3-ubyte.gz

if not os.path.exists("dataset/train-labels-idx1-ubyte.gz"):
  !wget -P dataset https://raw.githubusercontent.com/jskimn/aiservice_lesson/main/NNBasic/005_minist/dataset/train-labels-idx1-ubyte.gz

if not os.path.exists("dataset/t10k-images-idx3-ubyte.gz"):
  !wget -P dataset https://raw.githubusercontent.com/jskimn/aiservice_lesson/main/NNBasic/005_minist/dataset/t10k-images-idx3-ubyte.gz

if not os.path.exists("dataset/t10k-labels-idx1-ubyte.gz"):
  !wget -P dataset https://raw.githubusercontent.com/jskimn/aiservice_lesson/main/NNBasic/005_minist/dataset/t10k-labels-idx1-ubyte.gz

#sample weight 추가
if not os.path.exists("dataset/sample_weight.pkl"):
  !wget -P dataset https://raw.githubusercontent.com/jskimn/aiservice_lesson/main/NNBasic/005_minist/dataset/sample_weight.pkl

#확인
!realpath dataset/mnist.py
!ls dataset

#sys.path.append(os.pardir)
from dataset.mnist import load_mnist

(x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=False)

# 각 데이터 shape 출력
print(x_train.shape)
print(t_train.shape)
print(x_test.shape)
print(t_test.shape)

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt

def img_show(img):
    pil_img = Image.fromarray(np.uint8(img))
    #pil_img.show()
    plt.imshow(np.array(pil_img))

img = x_train[0]
print(img.shape)
label = t_train[0]
img = img.reshape(28, 28)
img_show(img)

print(label)

import sys, os
if not os.path.exists("dataset/mnist.py"):
  !wget -P dataset https://raw.githubusercontent.com/jskimn/aiservice_lesson/main/NNBasic/005_minist/dataset/mnist.py

if not os.path.exists("common/functions.py"):
  !wget -P common https://raw.githubusercontent.com/jskimn/aiservice_lesson/main/NNBasic/005_minist/common/functions.py

if not os.path.exists("dataset/train-images-idx3-ubyte.gz"):
  !wget -P dataset https://raw.githubusercontent.com/jskimn/aiservice_lesson/main/NNBasic/005_minist/dataset/train-images-idx3-ubyte.gz

if not os.path.exists("dataset/train-labels-idx1-ubyte.gz"):
  !wget -P dataset https://raw.githubusercontent.com/jskimn/aiservice_lesson/main/NNBasic/005_minist/dataset/train-labels-idx1-ubyte.gz

if not os.path.exists("dataset/t10k-images-idx3-ubyte.gz"):
  !wget -P dataset https://raw.githubusercontent.com/jskimn/aiservice_lesson/main/NNBasic/005_minist/dataset/t10k-images-idx3-ubyte.gz

if not os.path.exists("dataset/t10k-labels-idx1-ubyte.gz"):
  !wget -P dataset https://raw.githubusercontent.com/jskimn/aiservice_lesson/main/NNBasic/005_minist/dataset/t10k-labels-idx1-ubyte.gz

  #sample weight 추가
if not os.path.exists("dataset/sample_weight.pkl"):
  !wget -P dataset https://raw.githubusercontent.com/jskimn/aiservice_lesson/main/NNBasic/005_minist/dataset/sample_weight.pkl

  #확인
!realpath dataset/mnist.py
!ls dataset

from dataset.mnist import load_mnist
from common.functions import sigmoid, softmax
import numpy as np
import pickle

def get_data():
    (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, flatten=True, one_hot_label=False)
    return x_test, t_test

def init_network():
    with open("dataset/sample_weight.pkl", 'rb') as f:
        network = pickle.load(f)
    return network

def predict(network, x):
    W1, W2, W3 = network['W1'], network['W2'], network['W3']
    b1, b2, b3 = network['b1'], network['b2'], network['b3']

    a1 = np.dot(x, W1) + b1
    z1 = sigmoid(a1)
    a2 = np.dot(z1, W2) + b2
    z2 = sigmoid(a2)
    a3 = np.dot(z2, W3) + b3
    y = softmax(a3)

    return y

x, t = get_data()
network = init_network()
accuracy_cnt = 0
for i in range(len(x)):
    y = predict(network, x[i])
    p= np.argmax(y) # 확률이 가장 높은 원소의 인덱스를 얻는다.
    if p == t[i]:
        accuracy_cnt += 1

print("Accuracy:" + str(float(accuracy_cnt) / len(x)))

import sys, os
if not os.path.exists("dataset/mnist.py"):
  !wget -P dataset https://raw.githubusercontent.com/jskimn/aiservice_lesson/main/NNBasic/005_minist/dataset/mnist.py

if not os.path.exists("dataset/train-images-idx3-ubyte.gz"):
  !wget -P dataset https://raw.githubusercontent.com/jskimn/aiservice_lesson/main/NNBasic/005_minist/dataset/train-images-idx3-ubyte.gz

if not os.path.exists("dataset/train-labels-idx1-ubyte.gz"):
  !wget -P dataset https://raw.githubusercontent.com/jskimn/aiservice_lesson/main/NNBasic/005_minist/dataset/train-labels-idx1-ubyte.gz

if not os.path.exists("dataset/t10k-images-idx3-ubyte.gz"):
  !wget -P dataset https://raw.githubusercontent.com/jskimn/aiservice_lesson/main/NNBasic/005_minist/dataset/t10k-images-idx3-ubyte.gz

if not os.path.exists("dataset/t10k-labels-idx1-ubyte.gz"):
  !wget -P dataset https://raw.githubusercontent.com/jskimn/aiservice_lesson/main/NNBasic/005_minist/dataset/t10k-labels-idx1-ubyte.gz

#common 파일 다운르도
if not os.path.exists("common/functions.py"):
  !wget -P common https://raw.githubusercontent.com/jskimn/aiservice_lesson/main/NNBasic/005_minist/common/functions.py

if not os.path.exists("common/gradient.py"):
  !wget -P common https://raw.githubusercontent.com/jskimn/aiservice_lesson/main/NNBasic/005_minist/common/gradient.py

#TwoLayerNet.py 다운로드
if not os.path.exists("TwoLayerNet.py"):
  !wget https://raw.githubusercontent.com/jskimn/aiservice_lesson/main/NNBasic/005_minist/TwoLayerNet.py


#확인
!realpath dataset/mnist.py
!ls dataset

import numpy as np
import matplotlib.pyplot as plt
from dataset.mnist import load_mnist
from TwoLayerNet import TwoLayerNet

# 데이터 읽기
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)


# 하이퍼파라미터
iters_num = 10000  # 반복 횟수를 적절히 설정한다.
train_size = x_train.shape[0]
batch_size = 100   # 미니배치 크기
learning_rate = 0.1

#losss 및 정확도 변수
train_loss_list = []
train_acc_list = []
test_acc_list = []

# 1에폭당 반복 수
iter_per_epoch = max(train_size / batch_size, 1)

network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)

for i in range(iters_num):
    # 미니배치 획득
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]

    # 기울기 계산
    #grad = network.numerical_gradient(x_batch, t_batch)
    grad = network.gradient(x_batch, t_batch)

    # 매개변수 갱신
    for key in ('W1', 'b1', 'W2', 'b2'):
        network.params[key] -= learning_rate * grad[key]

    # 학습 경과 기록
    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss)

    # 1에폭당 정확도 계산
    if i % iter_per_epoch == 0:
        train_acc = network.accuracy(x_train, t_train)
        test_acc = network.accuracy(x_test, t_test)
        train_acc_list.append(train_acc)
        test_acc_list.append(test_acc)
        print("train acc, test acc | " + str(train_acc) + ", " + str(test_acc))

# 그래프 그리기
x = np.arange(len(train_loss_list))
plt.plot(x, train_loss_list)
plt.xlabel("iteration")
plt.ylabel("loss")
plt.ylim(0, 9)
plt.xlim(0, 10000)
plt.show()

x = np.arange(len(train_acc_list))
plt.plot(x, train_acc_list, label='train acc')
plt.plot(x, test_acc_list, label='test acc', linestyle='--')
plt.xlabel("epochs")
plt.ylabel("accuracy")
plt.ylim(0, 1.0)
plt.xlim(0, 16)
plt.legend(loc='lower right')
plt.show()

"""2025.04.01"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

transform = transforms.Compose([
    transforms.ToTensor(),           # PyTorch 텐서 변환
    transforms.Normalize((0.5,), (0.5,))  # 정규화 (-1 ~ 1)
])

train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)
test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)
train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False)

class TwoLayerNet(nn.Module):
    def __init__(self):
        super(TwoLayerNet, self).__init__()
        self.fc1 = nn.Linear(28*28, 50)  # 입력층 → 은닉층 (50 뉴런)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(50, 10)     # 은닉층 → 출력층 (10 뉴런)

    def forward(self, x):
        x = x.view(-1, 28*28)  # 2D 이미지 -> 1D 벡터 변환
        x = self.relu(self.fc1(x))
        x = self.fc2(x)  # Softmax는 CrossEntropyLoss에서 자동 적용됨
        return x

model = TwoLayerNet().to(device)
criterion = nn.CrossEntropyLoss() #손실함수: 교차 엔트로피 함수
optimizer = optim.SGD(model.parameters(), lr=0.1)  # 경사하강법

# 5. 모델 학습
num_epochs = 16
train_losses, test_accuracies = [], []

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0

    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    train_losses.append(running_loss / len(train_loader))

    # 테스트 정확도 계산
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            correct += (predicted == labels).sum().item()
            total += labels.size(0)

    test_acc = correct / total
    test_accuracies.append(test_acc)

    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {train_losses[-1]:.4f}, Test Accuracy: {test_acc:.4f}")

plt.plot(train_losses, label="Train Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()

plt.plot(test_accuracies, label="Test Accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
from datetime import datetime

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

learning_rate = 0.001
batch = 32
epochs = 15

image_size = 32
nb_class = 10

# transforms 정의하기
transforms = transforms.Compose([transforms.Resize((image_size, image_size)),
                                 transforms.ToTensor()])

# data set 다운받고 생성하기
train_dataset = datasets.MNIST(root='./data',
                               train=True,
                               transform=transforms,
                               download=True)

valid_dataset = datasets.MNIST(root='./data',
                               train=False,
                               transform=transforms)

# data loader 정의하기
train_loader = DataLoader(dataset=train_dataset,
                          batch_size=batch,
                          shuffle=True)

valid_loader = DataLoader(dataset=valid_dataset,
                          batch_size=batch,
                          shuffle=False)

class LeNet5(nn.Module):
    def __init__(self, n_classes):
        super(LeNet5, self).__init__()

        self.feature_extractor = nn.Sequential(
            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1),
            nn.Tanh(),
            nn.AvgPool2d(kernel_size=2),
            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1),
            nn.Tanh(),
            nn.AvgPool2d(kernel_size=2),
            nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5, stride=1),
            nn.Tanh()
        )
        self.classifier = nn.Sequential(
            nn.Linear(in_features=120, out_features=84),
            nn.Tanh(),
            nn.Linear(in_features=84, out_features=n_classes),
        )

    def forward(self, x):
        x = self.feature_extractor(x)
        x = torch.flatten(x, 1)
        logits = self.classifier(x)
        probs = F.softmax(logits, dim=1)
        return logits, probs

def get_accuracy(model, data_loader, device):
    correct_pred = 0
    n = 0

    with torch.no_grad():
        model.eval()
        for X, y_true in data_loader:

            X = X.to(device)
            y_true = y_true.to(device)

            _, y_prob = model(X)
            _, predicted_labels = torch.max(y_prob, 1)

            n += y_true.size(0)
            correct_pred += (predicted_labels == y_true).sum()

    return correct_pred.float() / n

def plot_losses(train_losses, valid_losses):
    train_losses = np.array(train_losses)
    valid_losses = np.array(valid_losses)

    fig, ax = plt.subplots(figsize = (8, 4.5))

    ax.plot(train_losses, color='blue', label='Training loss')
    ax.plot(valid_losses, color='red', label='Validation loss')
    ax.set(title="Loss over epochs",
            xlabel='Epoch',
            ylabel='Loss')
    ax.legend()
    fig.show()

    # plot style을 기본값으로 설정
    plt.style.use('default')

def train(train_loader, model, criterion, optimizer, device):
    model.train()
    running_loss = 0

    for X, y_true in train_loader:

        optimizer.zero_grad()

        X = X.to(device)
        y_true = y_true.to(device)

        # 순전파
        y_hat, _ = model(X)
        loss = criterion(y_hat, y_true)
        running_loss += loss.item() * X.size(0)

        # 역전파
        loss.backward()
        optimizer.step()

    epoch_loss = running_loss / len(train_loader.dataset)
    return model, optimizer, epoch_loss

def validate(valid_loader, model, criterion, device):
    model.eval()
    running_loss = 0

    for X, y_true in valid_loader:

        X = X.to(device)
        y_true = y_true.to(device)

        # 순전파와 손실 기록하기
        y_hat, _ = model(X)
        loss = criterion(y_hat, y_true)
        running_loss += loss.item() * X.size(0)

    epoch_loss = running_loss / len(valid_loader.dataset)

    return model, epoch_loss

def training_loop(model, criterion, optimizer, train_loader, valid_loader, epochs, device, print_every=1):
    # metrics를 저장하기 위한 객체 설정
    best_loss = 1e10
    train_losses = []
    valid_losses = []

    # model 학습하기
    for epoch in range(0, epochs):

        # training
        model, optimizer, train_loss = train(train_loader, model, criterion, optimizer, device)
        train_losses.append(train_loss)

        # validation
        with torch.no_grad():
            model, valid_loss = validate(valid_loader, model, criterion, device)
            valid_losses.append(valid_loss)

        if epoch % print_every == (print_every - 1):

            train_acc = get_accuracy(model, train_loader, device=device)
            valid_acc = get_accuracy(model, valid_loader, device=device)

            print(f'{datetime.now().time().replace(microsecond=0)} --- '
                  f'Epoch: {epoch}\t'
                  f'Train loss: {train_loss:.4f}\t'
                  f'Valid loss: {valid_loss:.4f}\t'
                  f'Train accuracy: {100 * train_acc:.2f}\t'
                  f'Valid accuracy: {100 * valid_acc:.2f}')

    plot_losses(train_losses, valid_losses)

    return model, optimizer, (train_losses, valid_losses)

#RANDOM_SEED = 42
#torch.manual_seed(RANDOM_SEED)

model = LeNet5(nb_class).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
criterion = nn.CrossEntropyLoss()


model, optimizer, _ = training_loop(model, criterion, optimizer, train_loader,
                                    valid_loader, epochs, device)

"""2025.04.08"""

#토큰나이저 설치
!pip install sentencepiece
#데이터 직렬화 util 설치
!pip install protobuf
#Hugging face 의 trasnformer 설치
!pip install transformers

# 예제 이미지 다운로드
!rm -rf ocr_test_image
!mkdir ocr_test_image
#!cd ocr_test_image && wget https://fki.tic.heia-fr.ch/static/img/a01-122-02.jpg
!cd ocr_test_image && wget https://github.com/jskimn/aiservice_lesson/raw/main/001_ocr/a01-122-02.jpg
!cd ocr_test_image && wget https://github.com/jskimn/aiservice_lesson/raw/main/001_ocr/Hello.jpg
!cd ocr_test_image && wget https://github.com/jskimn/aiservice_lesson/raw/main/001_ocr/test_01.png
!cd ocr_test_image && wget https://github.com/jskimn/aiservice_lesson/raw/main/001_ocr/test_02.png

from PIL import Image
from transformers import TrOCRProcessor, VisionEncoderDecoderModel
import matplotlib.pyplot as plt

# Load image
image = Image.open("./ocr_test_image/test_01.png").convert("RGB")

plt.figure(figsize=(10, 10))
plt.imshow(image)
plt.axis("on")
plt.show()

print("[INFO] Load pretrained TrOCRProcessor")
processor = TrOCRProcessor.from_pretrained("microsoft/trocr-base-handwritten")
print("[INFO] Load pretrained VisionEncoderDecoderModel")
model = VisionEncoderDecoderModel.from_pretrained("microsoft/trocr-base-handwritten")

# Preprocess
pixel_values = processor(images=image, return_tensors="pt").pixel_values
# Inference
token_ids = model.generate(pixel_values)
# Postprocess
text_from_image = processor.batch_decode(token_ids, skip_special_tokens=True)[0]

text_from_image

"""2025.04.29"""

!pip install git+https://github.com/facebookresearch/segment-anything.git
!pip install pycocotools==2.0.6

import os
import urllib

import cv2
import numpy as np
import matplotlib.pyplot as plt
import torch
from segment_anything import SamPredictor, sam_model_registry

CHECKPOINT_PATH = os.path.join("checkpoint")
CHECKPOINT_NAME = "sam_vit_h_4b8939.pth"
CHECKPOINT_URL = "https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

if not os.path.exists(CHECKPOINT_PATH):
    os.makedirs(CHECKPOINT_PATH, exist_ok=True)
checkpoint = os.path.join(CHECKPOINT_PATH, CHECKPOINT_NAME)
if not os.path.exists(checkpoint):
    urllib.request.urlretrieve(CHECKPOINT_URL, checkpoint)

sam = sam_model_registry["vit_h"](checkpoint=checkpoint).to(DEVICE)
predictor = SamPredictor(sam)

IMAGE_PATH = "mannequin.jpg"
image = cv2.imread(IMAGE_PATH, cv2.IMREAD_COLOR)
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

plt.figure(figsize=(10, 10))
plt.imshow(image)
plt.axis("on")
plt.show()

point_coords = np.array([[2073, 269]]) #첫번째 마네킹 : [1720, 230], 두번째 마네킹 : [2073, 269]
points_labels = np.array([1])

plt.figure(figsize=(10, 10))
plt.imshow(image)
plt.gca().scatter(
    point_coords[0, 0],
    point_coords[0, 1],
    color="green",
    marker="o",
    s=200,
    edgecolor="white",
    linewidth=1.25,
)
plt.axis("on")
plt.show()

predictor.set_image(image)
masks, scores, _ = predictor.predict(point_coords, points_labels)

for i, mask in enumerate(masks):
    print(f"Mask {i}, scores {scores[i]}")
    plt.figure(figsize=(10, 10))
    plt.imshow(mask,cmap='gray')  #cmap='viridis' 가 colormap 의 기본 값임, gray 로 변경해서 보자
    plt.axis("on")
    plt.show()

color = np.array([30/255, 144/255, 255/255, 0.5])

for mask in masks:
    mask_image = np.expand_dims(mask, axis=-1) * color.reshape(1, 1, -1)
    mask_image = (mask_image * 255).astype(np.uint8)

    plt.figure(figsize=(10, 10))
    plt.imshow(image)
    plt.imshow(mask_image)
    plt.axis("on")
    plt.show()

scores

mask = masks[np.argmax(scores)]

color = np.array([30/255, 144/255, 255/255, 0.5])
#4차원 mask image 생성을 위해 reshappe
mask_image = np.expand_dims(mask, axis=-1) * color.reshape(1, 1, -1)
mask_image = (mask_image * 255).astype(np.uint8)

plt.figure(figsize=(10, 10))
plt.imshow(image)
plt.imshow(mask_image)
plt.axis("on")
plt.show()

"""2025.05.13"""

!pip install -U openai-whisper

from IPython.display import Audio
from openai import OpenAI
import whisper

model = whisper.load_model("large")

audio_path="example/example.wav"
Audio(filename=audio_path)

result = model.transcribe(audio_path)
result["text"]

#Diffusion Models을 쉽게 사용하고 훈련할 수 있도록 도와주는 HuggingFace Library
!pip install diffusers

import torch
from diffusers import StableDiffusionImg2ImgPipeline
from diffusers.utils import make_image_grid
from PIL import Image

pipeline = StableDiffusionImg2ImgPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16,
).to("cuda")

input_image = Image.open("kitty.png")
make_image_grid([input_image], rows=1, cols=1)

prompt = "kitty in the city, cartoon style, drawing, detailed"
negative_prompt = "ugly, blurry, bad anatomy, bad art, wierd colors"

#num_images_per_prompt: 생성할 이미지 수
#num_inference_steps: 디퓨젼(노이즈 제거) 수행 횟수, 높을 수록 이미지 품질 높아짐
#strength: 입력 이미지의 정보 반영 비율, 높을 수록 반영 하지 않음.
output_images = pipeline(
    image=input_image,
    prompt=prompt,
    negative_prompt=negative_prompt,
    num_images_per_prompt=4,
    num_inference_steps=30,
    strength=0.7,
).images

make_image_grid(output_images, rows=1, cols=4)

!pip install diffusers

import torch
import os
import requests

from diffusers import StableDiffusionImg2ImgPipeline
from diffusers.utils import make_image_grid
from PIL import Image
from tqdm import tqdm

url = "https://civitai.com/models/65203/disney-pixar-cartoon-type-a"
model_id = url.replace("https://civitai.com/models/", "").split("/")[0]
print(model_id)

response = requests.get(f"https://civitai.com/api/v1/models/{model_id}")
response.json()

download_url = response.json()["modelVersions"][0]["downloadUrl"]
filename = response.json()["modelVersions"][0]["files"][0]["name"]
print("download_url:", download_url)
print("filename:", filename)


def download_from_url(url: str, file_path: str, chunk_size=1024):
    resp = requests.get(url, stream=True)
    total = int(resp.headers.get('content-length', 0))
    with open(file_path, 'wb') as file, tqdm(
        desc=file_path,
        total=total,
        unit='iB',
        unit_scale=True,
        unit_divisor=1024,
    ) as bar:
        for data in resp.iter_content(chunk_size=chunk_size):
            size = file.write(data)
            bar.update(size)


file_path = f"models/{filename}"
os.makedirs("models", exist_ok=True)
print(f"[INFO] Download start!")
download_from_url(download_url, file_path)
print(f"\n[INFO] File downloaded: {file_path}")

pipeline = StableDiffusionImg2ImgPipeline.from_single_file(
        "models/disneyPixarCartoon_v10.safetensors",
        torch_dtype=torch.float16,
        use_safetensors=True,
).to("cuda")

input_image = Image.open("kitty.png")

prompt = (
    "kitty, cartoon style, drawing, cat, orange cat, kitten"
)
negative_prompt = "ugly, blurry, bad anatomy, bad art, wierd colors"

output_images = pipeline(
    image=input_image,
    prompt=prompt,
    negative_prompt=negative_prompt,
    num_images_per_prompt=4,
    num_inference_steps=30,
    strength=0.7,
).images

make_image_grid(output_images, rows=1, cols=4)

